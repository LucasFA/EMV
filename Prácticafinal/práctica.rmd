---
title: "Práctica final"
author: "Agnese, Lucas Fehlau Arbulu"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: true
    toc_depth: 3
    number_sections: false
    toc_float: 
      collapsed: false
      smooth_scroll: false
  pdf_document:
    keep_tex: true
---
# Abstract
Diabetes, in Latin diabetes mellitus, is, worldwide, one of the largest public health problem and it seems to increase year after year. This is also why the National Institute of Diabetes and Digestive and Kidney Diseases every 2 years since 1965 analyses the evolution of diabetes in the Pima Indian population. In this paper we start with an univariate exploratory analysis and conclude with a multivariate analysis of the data, since the main goal is to understand which factors are the most relevant to rise the risks of diabetes.

# Introduction
In $2021$, more than $10.5\%$ of the world population apparently suffered from diabetes. The exact cause of this chronic illness is unknown, but several factors such as ethnicity, genetic inheritance, age, physical inactivity and excess of weight increase the risk of developing it. Diabetes results in high level of blood sugar as a consequence of insufficient pancreatic activity. In fact, it develops if the pancreas is not able to generate the appropriate amount of insulin or when the insulin produced is not used effectively by the organism. That is why, among other paramenters, in the study on the Pima population, the level of insulin 2 hours after glucose administration has been considered. The Pima Indians model of this illness is largely studied, because of the wide amount of data, which have been collected since the 1960s. The lifestyle of this population has changed and this is for sure one of the main purposes of the development of diabetes, but scientists suspect that as well a genetic factor played a great role. \
In this analysis we will now discuss using both a univariate and a multivariate explorative analysis, which factors most influence the onset of diabetes considering a sample of $768$ females of Pima Indian heritage, who are at least $21$ years old.

# Read the data
```{r ReadData}
diabetes_full <- read.csv("diabetes.csv", stringsAsFactors = TRUE)
head(diabetes_full)
```

# Univariate explorative analysis
The variables we are dealing with are described as below: 

+ number of times pregnant;
+ plasma glucose concentration a 2 hours in an oral glucose tolerance test;
+ diastolic blood pressure (mmHg);
+ triceps skin fold thickness (mm);
+ 2-Hour serum insulin ($\mu$U/mL);
+ body mass index (weight in kg/(height in m)^2);
+ diabetes pedigree function;
+ age (years);
+ class variable (0 or 1 for tested_positive or tested_negative).

We are not interesed in the number of pregnancies the patient has had, so we will omit this variable. The response variable will be `class`, which tells us if a patient shows signs of diabetes according to World Health Organization criteria (i.e., if the 2 hour post-load plasma glucose was at least $200$ mg/dl at any survey examination or if found during routine medical care). A sample of $768$ females of Pima Indian heritage, a population that lives near Phoenix, Arizona, USA, have been considered, where the girls are at least $21$ years old.\

```{r}
# Remove the first column from the dataset
diabetes <- diabetes_full[, -1]
head(diabetes)
```

In this dataset, all the missing values have been substituted by the number $0$. Since, for example, it doesn't make any sense to consider a $0$ mm thickness of the triceps skin fold of a girl or a zero diastolic blood pressure, we detect this ``kind of" missing values and we check if it would be appropriate to substitute them with the mean of the considered variable. 
```{r PrecentNA, echo=TRUE}
# Replace zeros of all columns with NA, so that we can use the functions to
# detect missing values showed in class
diabetes[diabetes == 0] <- NA
head(diabetes)

### ----- i) -----
# Percentage of missing values in each column
percNA <- (colMeans(is.na(diabetes))) * 100
percNA
# Columns skin and insu have more then 5%
```
The percentage of missing values for the variables `skin` and `insu` is greater then $5\%$, so let's check if it makes sense to substitute that values by the mean as described above.\
To prove this, we test the homogenity of the columns with a Student t-test and with the chi-squared test of independece, when we deal with the last colum (since it is a discrete variable).

```{r IndepTests, echo=TRUE}

### ------- ii) ----------
# Student t-test to check homogenity between columns/variables
t.test(diabetes$plas, diabetes$skin)
t.test(diabetes$pres, diabetes$skin)
t.test(diabetes$insu, diabetes$skin)
t.test(diabetes$mass, diabetes$skin)
t.test(diabetes$pedi, diabetes$skin)
t.test(diabetes$age, diabetes$skin)

# Chi-squared test to check independence between the discrete column class and the column skin.
chisq.test(diabetes$class, diabetes$skin)
```

The p-value obtained in each test is smaller than the significance level $\alpha= 0.05$, therefore we would statistically reject the homogenity between the variables and we would not substitute the missing values with the mean. Anyway, after a discussion with the lead researcher on the project, we replace the missing values by the mean of the respective column and we obtain a ``clean" dataset.

```{r Outiliers, echo=TRUE}
# Function to detect missing values and replace them with the mean of that
# variable
not_available <- function(data) {
  data[is.na(data)] <- mean(data, na.rm = T)
  data
}

# Replace NA with the mean of that variable
# Only the first 7 columns have been considered, since they contain numerical values.
# The last one is a 'character'-column, so we cannot apply the function
# to dectect missing values, because there aren't any there.
df <- data.frame(lapply(diabetes[1:7], not_available))
head(df)

# Data without NA. The NA are replaced by the mean of each column.
new_diab <- cbind(df, diabetes[8])
head(new_diab)
```

In the following table, the main statistical characteristics of the dataset have been summarized.

```{r Table, echo = FALSE}
# c) Exploratory analysis
library(moments)
library(kableExtra)
library(latex2exp)

# Summary of the data
summaryData <- summary(new_diab[1:7])
s <- matrix(c(as.numeric(sub(".*:", "", summaryData))), ncol = 7, byrow = FALSE)

# Standard deviations
sd_plas <- sd(new_diab$plas)
sd_pres <- sd(new_diab$pres)
sd_skin <- sd(new_diab$skin)
sd_insu <- sd(new_diab$insu)
sd_mass <- sd(new_diab$mass)
sd_pedi <- sd(new_diab$pedi)
sd_age <- sd(new_diab$age)

# Skewness
skew_plas <- skewness(new_diab$plas)
skew_pres <- skewness(new_diab$pres)
skew_skin <- skewness(new_diab$skin)
skew_insu <- skewness(new_diab$insu)
skew_mass <- skewness(new_diab$mass)
skew_pedi <- skewness(new_diab$pedi)
skew_age <- skewness(new_diab$age)

# Kurtosis
kurt_plas <- kurtosis(new_diab$plas)
kurt_pres <- kurtosis(new_diab$pres)
kurt_skin <- kurtosis(new_diab$skin)
kurt_insu <- kurtosis(new_diab$insu)
kurt_mass <- kurtosis(new_diab$mass)
kurt_pedi <- kurtosis(new_diab$pedi)
kurt_age <- kurtosis(new_diab$age)


# Create the table

tabl <- matrix(c(
  s[4, 1], s[4, 2], s[4, 3], s[4, 4], s[4, 5], s[4, 6], s[4, 7],
  sd_plas, sd_pres, sd_skin, sd_insu, sd_mass, sd_pedi, sd_age,
  s[2, 1], s[2, 2], s[2, 3], s[2, 4], s[2, 5], s[2, 6], s[2, 7],
  s[3, 1], s[3, 2], s[3, 3], s[3, 4], s[3, 5], s[3, 6], s[3, 7],
  s[5, 1], s[5, 2], s[5, 3], s[5, 4], s[5, 5], s[5, 6], s[5, 7],
  skew_plas, skew_pres, skew_skin, skew_insu, skew_mass, skew_pedi, skew_age,
  kurt_plas, kurt_pres, kurt_skin, kurt_insu, kurt_mass, kurt_pedi, kurt_age
),
ncol = 7, byrow = TRUE
)

colnames(tabl) <- c(
  "Plasma glucose concentration", "Diastolic blood pressure",
  "Triceps thickness", "Insulin", "BMI", "Pedigree function", "Age"
)

rownames(tabl) <- c("Mean", "Standard deviation", "25%-Quantile", "Median", "75%-Quantile", "Skewness", "Kurtosis")

tabl <- as.table(tabl)

options(knitr.kable.NA = "---")
kable(tabl, digits = 3, align = rep("c", 7), caption = "Main statistical characteristics") %>%
  kable_styling(latex_options = c("scale_down", "hold_position"))
```
The data are collected in the appropriate, but of course different, measuring scales, so in order to be able to compare them with boxplots, we need to standardize the data.
```{r BoxplotStand, echo=TRUE}
# Standardize the variables
v_plas <- (new_diab[1] - s[4, 1]) / sd_plas
v_pres <- (new_diab[2] - s[4, 2]) / sd_pres
v_skin <- (new_diab[3] - s[4, 3]) / sd_skin
v_insu <- (new_diab[4] - s[4, 4]) / sd_insu
v_mass <- (new_diab[5] - s[4, 5]) / sd_mass
v_pedi <- (new_diab[6] - s[4, 6]) / sd_pedi
v_age <- (new_diab[7] - s[4, 7]) / sd_age

# Boxplot of the data
standardData <- cbind(v_plas, v_pres, v_skin, v_insu, v_mass, v_pedi, v_age)
boxplot(standardData, col = cm.colors(7))
```

All variables present outliers, except the one that measures the plasma glucose concentracion. We try to eliminate the outliers and substitute this values by the mean of the column.

```{r outliers, echo=TRUE, include=TRUE, warning=FALSE }
# Function for finding the outliers
outlier <- function(data) {
  H <- 1.5 * IQR(data)
  data[data < quantile(data, 0.25, na.rm = T) - H] <- NA
  data[data > quantile(data, 0.75, na.rm = T) + H] <- NA
  data[is.na(data)] <- mean(data, na.rm = T)
  H <- 1.5 * IQR(data)
  if (TRUE %in% (data < quantile(data, 0.25, na.rm = T) - H) |
    TRUE %in% (data > quantile(data, 0.75, na.rm = T) + H)) {
    outlier(data)
  } else {
    return(data)
  }
}

# Find the outliers in the dataset and replace them with the appropriate mean
no_out_data <- as.data.frame(apply(new_diab[1:7], 2, outlier))

# Standardize now the data without outliers
no_out.plas <- (no_out_data$plas - mean(no_out_data$plas)) / sd(no_out_data$plas)
no_out.pres <- (no_out_data$pres - mean(no_out_data$pres)) / sd(no_out_data$pres)
no_out.skin <- (no_out_data$skin - mean(no_out_data$skin)) / sd(no_out_data$skin)
no_out.insu <- (no_out_data$insu - mean(no_out_data$insu)) / sd(no_out_data$insu)
no_out.mass <- (no_out_data$mass - mean(no_out_data$mass)) / sd(no_out_data$mass)
no_out.pedi <- (no_out_data$pedi - mean(no_out_data$pedi)) / sd(no_out_data$pedi)
no_out.age <- (no_out_data$age - mean(no_out_data$age)) / sd(no_out_data$age)

no_out_standard <- cbind(
  no_out.plas, no_out.pres, no_out.skin, no_out.insu,
  no_out.mass, no_out.pedi, no_out.age
)

# Boxplots without outliers
boxplot(no_out_standard,
  main = "Boxplots without outliers",
  names = c("plas", "pres", "skin", "insu", "mass", "pedi", "age"),
  col = cm.colors(7)
)
```

Even if we replaced the outliers by the mean and we standardized the values, we see that we couldn't get rid of all of them for the blood pressure. One of the main purposes, according to the lead researcher, is the genetic inheritance: women with a hight blood pressure probabily had parents with hight blood pressure as well. We note also that this variable takes the smalles and the highest value between all columns. The variable `insu` is the one with the larger variability, since the quantity of serum insulin depends on how the organism absorbes the glucose, while for all the others parameters the variability is almost the same.\

We analyse if the data are normally distributed. For a perfect normal distribution, the skewness and the kurtosis should be zero, but, in our case, non of corresponding value for each column is zero. So let's see if the normality assumption could be appropriate by looking at the following QQ-plots.

```{r qq plot, echo = FALSE, fig.width = 13, fig.height = 5}

## QQ-Plots
par(mfrow = c(2, 4))

# Plasma glucose concentration
qqnorm(no_out.plas,
  main = "Plasma glucose concentration",
  col = "cyan3",
  cex.lab = 1.2, cex.main = 1.5,
  xlab = "Theoretical quantiles (normal distribution)",
  ylab = "Empirical quantiles"
)
qqline(no_out.plas, distribution = qnorm)

h_plas <- hist(no_out.plas,
  col = "cyan3",
  ylab = "Frequency",
  main = "",
  cex.lab = 1.2
)
xfit <- seq(min(no_out.plas), max(no_out.plas), length = 100)
yfit <- dnorm(xfit, mean = mean(no_out.plas), sd = sd(no_out.plas))
yfit <- yfit * diff(h_plas$mids[1:2]) * length(no_out.plas)
lines(xfit, yfit, col = "black", lwd = 2)

# Diastolic blood pressure
qqnorm(no_out.pres,
  main = "Diastolic blood pressure",
  col = "orange1",
  cex.lab = 1.2, cex.main = 1.5,
  xlab = "Theoretical quantiles (normal distribution)",
  ylab = "Empirical quantiles"
)
qqline(no_out.pres, distribution = qnorm)

h_pres <- hist(no_out.pres,
  col = "orange1",
  ylab = "Frequency",
  main = "",
  cex.lab = 1.2
)
xfit <- seq(min(no_out.pres), max(no_out.pres), length = 100)
yfit <- dnorm(xfit, mean = mean(no_out.pres), sd = sd(no_out.pres))
yfit <- yfit * diff(h_pres$mids[1:2]) * length(no_out.pres)
lines(xfit, yfit, col = "black", lwd = 2)

# Triceps thickness
qqnorm(no_out.skin,
  main = "Triceps thickness",
  col = "chartreuse2",
  cex.lab = 1.2, cex.main = 1.5,
  xlab = "Theoretical quantiles (normal distribution)",
  ylab = "Empirical quantiles",
)
qqline(no_out.skin, distribution = qnorm)

h_skin <- hist(no_out.skin,
  col = "orchid2",
  ylab = "chartreuse2",
  main = "",
  cex.lab = 1.2
)
xfit <- seq(min(no_out.skin), max(no_out.skin), length = 100)
yfit <- dnorm(xfit, mean = mean(no_out.skin), sd = sd(no_out.skin))
yfit <- yfit * diff(h_skin$mids[1:2]) * length(no_out.skin)
lines(xfit, yfit, col = "black", lwd = 2)

# Insulin
qqnorm(no_out.insu,
  main = "Insulin",
  col = "cornflowerblue",
  cex.lab = 1.2, cex.main = 1.5,
  xlab = "Theoretical quantiles (normal distribution)",
  ylab = "Empirical quantiles",
)
qqline(no_out.insu, distribution = qnorm)

h_insu <- hist(no_out.insu,
  col = "cornflowerblue",
  ylab = "Frequency",
  main = "",
  cex.lab = 1.2
)
xfit <- seq(min(no_out.insu), max(no_out.insu), length = 100)
yfit <- dnorm(xfit, mean = mean(no_out.insu), sd = sd(no_out.insu))
yfit <- yfit * diff(h_insu$mids[1:2]) * length(no_out.insu)
lines(xfit, yfit, col = "black", lwd = 2)

# Body mass index
qqnorm(no_out.mass,
  main = "Body mass index",
  col = "blueviolet",
  cex.lab = 1.2, cex.main = 1.5,
  xlab = "Theoretical quantiles (normal distribution)",
  ylab = "Empirical quantiles",
)
qqline(no_out.mass, distribution = qnorm)

h_mass <- hist(no_out.mass,
  col = "blueviolet",
  ylab = "Frequency",
  main = "",
  cex.lab = 1.2
)
xfit <- seq(min(no_out.mass), max(no_out.mass), length = 100)
yfit <- dnorm(xfit, mean = mean(no_out.mass), sd = sd(no_out.mass))
yfit <- yfit * diff(h_mass$mids[1:2]) * length(no_out.mass)
lines(xfit, yfit, col = "black", lwd = 2)

# Pedigree function
qqnorm(no_out.pedi,
  main = "Pedigree function",
  col = "orchid2",
  cex.lab = 1.2, cex.main = 1.5,
  xlab = "Theoretical quantiles (normal distribution)",
  ylab = "Empirical quantiles",
)
qqline(no_out.pedi, distribution = qnorm)

h_pedi <- hist(no_out.pedi,
  col = "orchid2",
  ylab = "Frequency",
  main = "",
  cex.lab = 1.2
)
xfit <- seq(min(no_out.pedi), max(no_out.pedi), length = 100)
yfit <- dnorm(xfit, mean = mean(no_out.pedi), sd = sd(no_out.pedi))
yfit <- yfit * diff(h_pedi$mids[1:2]) * length(no_out.pedi)
lines(xfit, yfit, col = "black", lwd = 2)

# Age
qqnorm(no_out.age,
  main = "Age",
  col = "coral1",
  cex.lab = 1.2, cex.main = 1.5,
  xlab = "Theoretical quantiles (normal distribution)",
  ylab = "Empirical quantiles",
)
qqline(no_out.age, distribution = qnorm)

h_age <- hist(no_out.age,
  col = "coral1",
  ylab = "Frequency",
  main = "",
  cex.lab = 1.2
)
xfit <- seq(min(no_out.age), max(no_out.age), length = 100)
yfit <- dnorm(xfit, mean = mean(no_out.age), sd = sd(no_out.age))
yfit <- yfit * diff(h_age$mids[1:2]) * length(no_out.age)
lines(xfit, yfit, col = "black", lwd = 2)
```

By looking at the QQ-plots, it seems that only the diastolic blood pressure and the body mass index follow a normal distribution. This is as well confirmed by the histogram of the corresponding variables: the bars of the histogram follow roughly a bell-shaped curve and are symmetric around zero. Also the histogram of the plasma glucose concentration approximate to a normal distribution, but the QQ-plot exibits an ``S-shape" and the points are not following a stright line (which is what we need in case of normality).\
To conclude this first part, we test for normality with a Shapiro-Wilk test.
```{r TestNormality, echo=TRUE}

# The null hypothesis of these tests is that “sample distribution is normal”. If the test is significant, the distribution is non-normal.
# All tests reject, so all the variables are non-normal

shapiro.test(no_out.plas)
shapiro.test(no_out.pres)
shapiro.test(no_out.skin)
shapiro.test(no_out.insu)
shapiro.test(no_out.mass)
shapiro.test(no_out.pedi)
shapiro.test(no_out.age)
```

As expected, all tests reject the null hypothesis of normality in favor of the alternative. With a significance level of $\alpha = 0.05$ we can assume that none of the variable is normally distributed. Interesting is to note that, when testing the normality of the blood pressure the p-value obtained is `r round(as.numeric(shapiro.test(no_out.pres)$p.value), digits = 4)`. This suggest that if we made a test at a significance level of $0.01$, we would not have rejected the null hypotesis so we would have assumed that the variable `pres` is normally distributed. For all the other cases, the p-values are smaller then the $0.001$ so we would have rejected normality in every case. \
Finally, with an histogram we visualized that $500$ females out of $768$ have been tested negative, while the remaining $268$ present signs of diabetes.

```{r HistResponse, echo=TRUE}
hist(as.numeric(unlist(new_diab[8])),
  col = "coral1",
  xlab = "tested positive - tested negative",
  ylab = "Frequency",
  main = "Histogram tested positive - tested negative",
  cex.lab = 1.2
)
```


# Multivariate exploratory analysis

<!-- a) -->
We will use the Bartlett test to check if the variables are homoscedastic. 

```{r}
diab_no_output <- subset(new_diab, select = -class)
cor(diab_no_output)
library(psych)
data_normalised <- scale(diab_no_output)
cortest.bartlett(cor(data_normalised))
```
<!-- b hecho? TODO -->
<!-- c Done in the previous  -->


<!-- d) -->
## PCA
```{r}
PCA <- prcomp(diab_no_output, scale = TRUE, center = TRUE)

PCA$rotation

plot(cumsum(PCA$sdev^2) / (sum(PCA$sdev^2)), type = "l") # check this
summary(PCA)
```
### Choosing the number of components
We need the function `fviz_screeplot` from the `factoextra` package.

```{r}
library(factoextra)
```

#### Elbow method
```{r}
fviz_screeplot(PCA, addlabels = TRUE)
```
Suggests 3 components

#### Mean variace method
```{r}
PCA$sdev
mean(PCA$sdev^2)
```
Also suggests 3 components.

<!-- TODO: where does this go? It doesn't seem to appear in the guide -->
### Visualization of principal components
```{r}
fviz_pca_var(PCA,
  repel = TRUE, col.var = "cos2",
  legend.title = "Distancia"
) + theme_bw()
```

```{r}
fviz_pca_var(PCA,
  axes = c(1, 3),
  repel = TRUE, col.var = "cos2",
  legend.title = "Distancia"
) + theme_bw()
```

<!-- TODO?: interpretation -->

<!-- e) -->
## Factor analysis
### Preconditions
We saw in our previous analysis of correlations for PCA that the data 

```{r}
poly_cor <- polycor::hetcor(diab_no_output)$correlations
ggcorrplot::ggcorrplot(poly_cor, type = "lower", hc.order = T)
```

```{r}
corrplot::corrplot(cor(diab_no_output), order = "hclust", tl.col = "black", tl.cex = 1)
```

### Factor 

MLE model:
```{r}
model1 <- fa(poly_cor,
  nfactors = 3,
  rotate = "none",
  fm = "mle"
)
```
Minimum residual model:
```{r}
model2 <- fa(poly_cor,
  nfactors = 3,
  rotate = "none",
  fm = "minres"
)
```
Compare the communalities
```{r }
c1 <- sort(model1$communality, decreasing = TRUE)
c2 <- sort(model2$communality, decreasing = TRUE)
head(cbind(c1, c2))
```

<!-- TODO: check the name of this -->
Compare the unicities 

```{r }
u1 <- sort(model1$uniquenesses, decreasing = TRUE)
u2 <- sort(model2$uniquenesses, decreasing = TRUE)
head(cbind(u1, u2))
```

### Determine the number of factors
```{r }
scree(poly_cor)
fa.parallel(poly_cor, n.obs = length(diab_no_output[, 1]), fa = "fa", fm = "minres")
```

Both plots suggest using 3 factors.

### Model
```{r }
varimax_model <- fa(poly_cor,
  nfactors = 3, rotate = "varimax",
  fa = "mle"
)

print(varimax_model$loadings, cut = 0)
fa.diagram(varimax_model)
stats::factanal(diab_no_output, factors = 3, rotation = "none")
```

### Note
Notice that we receive a warning about this being an Ultra-Heywood case. According to the literature, this means the model is not reliable. In short, it is a case in which the communalities exceed $1$. There could be several reasons for this.

<!-- f -->
## Multivariate normality testing

<!-- TODO: adapt to our use case -->

### Marginal distributions
```{r}
par(mfrow = c(2, 4))
for (k in seq_len(length(new_diab) - 1)) {
  variable_name <- names(new_diab)[k]
  for (i in 1:2) {
    output_result <- levels(new_diab$class)[i]
    x <- new_diab[new_diab$class == output_result, variable_name]
    hist(x,
      breaks = 20,
      main = paste("Histogram", variable_name, "for", output_result),
      xlab = variable_name,
      col = i + 1
    )
  }
}
```

### Gráficos QQ-plot
```{r}
# Representación de cuantiles normales de cada variable para cada especie
par(mfrow = c(2, 4))
for (k in seq_len(length(new_diab) - 1)) {
  name_variable <- names(new_diab)[k]
  some_seq <- seq(min(new_diab[, k]), max(new_diab[, k]), le = 50)
  for (i in 1:2) {
    output_result <- levels(new_diab$class)[i]
    x <- new_diab[new_diab$class == output_result, name_variable]
    qqnorm(x,
      main = paste("QQ-plot", name_variable, "for", output_result),
      pch = 19,
      col = i + 1)
    qqline(x)
  }
}
par(mfrow = c(1, 1))
```

In addition to this, we previously saw that the data was not normally distributed according to the Shapiro-Wilk test.


#### Multivariate

Due to the sensibility of the multivariate normality test to outliers, we check again: 
```{r}
outliers <- MVN::mvn(data = diab_no_output, mvnTest = "hz", multivariateOutlierMethod = "quan")
```

And with them removed we use two tests to check for normality

```{r}
royston_test <- MVN::mvn(data = diab_no_output, mvnTest = "royston", multivariatePlot = "qq")

royston_test$multivariateNormality

hz_test <- MVN::mvn(data = diab_no_output, mvnTest = "hz")
hz_test$multivariateNormality
```
<!-- g -->
## Classifier

```{r}
pairs(
  x = diab_no_output[sample(nrow(diab_no_output), 100), ],
  col = c("green", "red")[new_diab$class],
  pch = 19
)
```

#### Homogeneity of variance
```{r}
biotools::boxM(
  data = new_diab[1:7],
  grouping = new_diab[, 8]
)
```

### Discriminants

#### Training-test split

```{r}
set.seed(42)
training_rows <- sample(
  nrow(new_diab),
  size = 0.75 * nrow(new_diab),
  replace = TRUE
)
training_data <- new_diab[training_rows, ]
test_data <- new_diab[-training_rows, ]
```

#### Linear Discriminant
```{r}
lda_model <- MASS::lda(formula = class ~ plas + pres + skin + insu + mass + pedi + age, data = training_data)
lda_model
```

#### Cross-validation

```{r}
# In sample error
in_sample_pred <- predict(lda_model, training_data)
biotools::confusionmatrix(training_data$class, in_sample_pred$class)
training_error <- mean(training_data$class != in_sample_pred$class) * 100
paste("In sample error = ", training_error, "%")

# Out of sample error
pred <- predict(lda_model, test_data)
biotools::confusionmatrix(test_data$class, pred$class)

test_error <- mean(test_data$class != pred$class) * 100
paste("Out of sample error = ", test_error, "%")
```

Due to the high number of dimensions and the high error rate in classification, visualizing the data is not very insightful in this case.

### Quadratic Discriminant

```{r}
qda_model <- MASS::qda(formula = class ~ plas + pres + skin + insu + mass + pedi + age, data = training_data)
qda_model
```

#### Cross-validation
```{r}
# In sample error
in_sample_pred <- predict(qda_model, training_data)
biotools::confusionmatrix(training_data$class, in_sample_pred$class)
training_error <- mean(training_data$class != in_sample_pred$class) * 100
paste("In sample error = ", training_error, "%")

# Out of sample error
pred <- predict(qda_model, test_data)
biotools::confusionmatrix(test_data$class, pred$class)
test_error <- mean(test_data$class != pred$class) * 100
paste("Out of sample error = ", test_error, "%")
```

### Conclusion

Despite the fact that the data is not normally distributed, both the LDA and QDA models show a possible discriminant, albeit with a high error rate. The LDA model has a slightly lower out-of-sample error, but the difference is not significant. In addition, it is simpler and thus easier to interpret.

